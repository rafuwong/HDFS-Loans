{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1804d554-8938-4d26-892b-0498ec79c37f",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ab6eab-4218-4f9d-8dbb-dc45c47404de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pyarrow as pa\n",
    "import pyarrow.fs\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7299a61b-4e73-4d2d-8477-51fd9598e3cb",
   "metadata": {},
   "source": [
    "# Part 1: Deployment and Data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ab091a3-1017-4694-9352-fd312f0c4c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 51642105856 (48.10 GB)\n",
      "Present Capacity: 31571872892 (29.40 GB)\n",
      "DFS Remaining: 31042887680 (28.91 GB)\n",
      "DFS Used: 528985212 (504.48 MB)\n",
      "DFS Used%: 1.68%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (2):\n",
      "\n",
      "Name: 172.24.0.2:9866 (project-4-p4-rfwong07-dn-2.project-4-p4-rfwong07_default)\n",
      "Hostname: 4011abf154e0\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 25821052928 (24.05 GB)\n",
      "DFS Used: 268276021 (255.85 MB)\n",
      "Non DFS Used: 10014555851 (9.33 GB)\n",
      "DFS Remaining: 15521443840 (14.46 GB)\n",
      "DFS Used%: 1.04%\n",
      "DFS Remaining%: 60.11%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Sat Mar 16 20:48:32 GMT 2024\n",
      "Last Block Report: Sat Mar 16 20:44:47 GMT 2024\n",
      "Num of Blocks: 254\n",
      "\n",
      "\n",
      "Name: 172.24.0.5:9866 (project-4-p4-rfwong07-dn-1.project-4-p4-rfwong07_default)\n",
      "Hostname: 661631f2ec8b\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 25821052928 (24.05 GB)\n",
      "DFS Used: 260709191 (248.63 MB)\n",
      "Non DFS Used: 10022122681 (9.33 GB)\n",
      "DFS Remaining: 15521443840 (14.46 GB)\n",
      "DFS Used%: 1.01%\n",
      "DFS Remaining%: 60.11%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Sat Mar 16 20:48:32 GMT 2024\n",
      "Last Block Report: Sat Mar 16 20:44:47 GMT 2024\n",
      "Num of Blocks: 247\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#q1\n",
    "! hdfs dfsadmin -fs hdfs://boss:9000 -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aaf8a17-60b6-4286-9d43-244b52cc06b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write code that downloads  https://pages.cs.wisc.edu/~harter/cs544/data/hdma-wi-2021.csv if\n",
    "# it hasn't been already downloaded\n",
    "if not os.path.exists('hdma-wi-2021.csv'):\n",
    "    ! wget https://pages.cs.wisc.edu/~harter/cs544/data/hdma-wi-2021.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b9de295-4d80-4862-a394-6eb703d96fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://boss:9000/single.csv\n",
      "Deleted hdfs://boss:9000/double.csv\n"
     ]
    }
   ],
   "source": [
    "#Remove the single.csv and double.csv to avoid clutter/duplicates\n",
    "!hdfs dfs -rm -f hdfs://boss:9000/single.csv\n",
    "!hdfs dfs -rm -f hdfs://boss:9000/double.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daad5ce9-f232-4cdd-82fe-64823796bdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next use `hdfs dfs -cp` to copy to cluster\n",
    "# !hdfs dfs -mkdir hdfs://boss:9000/data\n",
    "!hdfs dfs -Ddfs.block.size=1048576 -Ddfs.replication=1 -cp hdma-wi-2021.csv hdfs://boss:9000/single.csv\n",
    "!hdfs dfs -Ddfs.block.size=1048576 -Ddfs.replication=2 -cp hdma-wi-2021.csv hdfs://boss:9000/double.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06d91869-17e2-42b3-90f0-228f68d0fc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166.8 M  333.7 M  hdfs://boss:9000/double.csv\n",
      "166.8 M  166.8 M  hdfs://boss:9000/single.csv\n"
     ]
    }
   ],
   "source": [
    "#q2\n",
    "!hdfs dfs -du -h hdfs://boss:9000/ #Run a du command with hdfs dfs to see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f687a4ec-cee0-41f6-a375-268ff98b5f01",
   "metadata": {},
   "source": [
    "# Part 2: WebHDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15dedc50-74e6-48cd-bca7-70acfe41b2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FileStatus': {'accessTime': 1710622125129,\n",
       "  'blockSize': 1048576,\n",
       "  'childrenNum': 0,\n",
       "  'fileId': 16388,\n",
       "  'group': 'supergroup',\n",
       "  'length': 174944099,\n",
       "  'modificationTime': 1710622129054,\n",
       "  'owner': 'root',\n",
       "  'pathSuffix': '',\n",
       "  'permission': '644',\n",
       "  'replication': 1,\n",
       "  'storagePolicy': 0,\n",
       "  'type': 'FILE'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "namenode_url = \"http://boss:9870/webhdfs/v1/single.csv\"\n",
    "getfilestatus_url = f\"{namenode_url}?op=GETFILESTATUS\" #Create the url with ?op=GETFILESTATUS \n",
    "r = requests.get(getfilestatus_url)\n",
    "filestatus_dict = r.json() #Answer with a dictionary with json\n",
    "filestatus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bff0599-ff4f-4bc6-bbb3-4f0d006234c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://4011abf154e0:9864/webhdfs/v1/single.csv?op=OPEN&namenoderpcaddress=boss:9000&offset=0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4\n",
    "namenode_url = \"http://boss:9870/webhdfs/v1/single.csv\"\n",
    "location_url = f\"{namenode_url}?op=OPEN&offset=0&noredirect=true\" #OPEN Operation offset=0 noredirect=true\n",
    "r = requests.get(location_url)\n",
    "location_string = list(r.json().values())[0] #Could also do r.json()['Location']\n",
    "location_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a89190b-7e15-4c84-828d-3b4cc4cdd943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4011abf154e0': 90, '661631f2ec8b': 77}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5\n",
    "namenode_url = \"http://boss:9870/webhdfs/v1/single.csv\"\n",
    "location_url = f\"{namenode_url}?op=GETFILEBLOCKLOCATIONS\" #Reference https://hadoop.apache.org/docs/r3.3.6/hadoop-project-dist/hadoop-hdfs/WebHDFS.html#Get_File_Block_Locations\n",
    "r = requests.get(location_url)\n",
    "block_location_dict = r.json()\n",
    "\n",
    "block_location_list = block_location_dict['BlockLocations']['BlockLocation']\n",
    "\n",
    "\n",
    "hosts_count_dict = {} #Initiate counting dictionary\n",
    "for item in block_location_list: #This loops through each item in the list and count up the \"hosts\" value\n",
    "    cntr_value = item['hosts'][0]\n",
    "    if cntr_value not in hosts_count_dict:\n",
    "        hosts_count_dict[cntr_value] = 1 #if cntr id not in start key-value pair and start count at 1\n",
    "    else:\n",
    "        hosts_count_dict[cntr_value] += 1\n",
    "hosts_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30bf0f1-c7bd-409e-a169-9ca0e740c6bb",
   "metadata": {},
   "source": [
    "# Part 3 Pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2acf29b-be5f-4b35-9a1e-c0b267b42bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-16 20:49:05,624 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'activity_y'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q6 \n",
    "hdfs = pa.fs.HadoopFileSystem(\"boss\",9000)\n",
    "\n",
    "#Open single.csv file with HDFS\n",
    "#Referenced from lec/18-hdfs/nb/hdfs.ipynb for 'with as f' portion\n",
    "with hdfs.open_input_file('/single.csv') as f:\n",
    "    first_10_bytes = f.read_at(10, 0) #Read first 10 bytes offset at 0\n",
    "first_10_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc2769ee-be70-4a33-90cc-83c860642723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444874"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q7\n",
    "#Referenced from lec/18-hdfs/nb/hdfs.ipynb\n",
    "single_family_count = 0 #Counter for \"Single Family\" string\n",
    "with hdfs.open_input_file('/single.csv') as f:\n",
    "    reader = io.TextIOWrapper(io.BufferedReader(f))\n",
    "    for i, line in enumerate(reader):\n",
    "        # print(line, end=\"\")\n",
    "        if \"Single Family\" in line:\n",
    "            single_family_count += 1\n",
    "        # if i > 10: #Debugging purposes for just the first couple of lines\n",
    "        #     break\n",
    "single_family_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
